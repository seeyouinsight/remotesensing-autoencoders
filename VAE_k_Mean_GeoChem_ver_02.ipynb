{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE_k-Mean_GeoChem_ver_02.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Import necessary libraries"
      ],
      "metadata": {
        "id": "5JdAlthqWxKx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOFoLUz3WtnZ"
      },
      "outputs": [],
      "source": [
        "# pip install rasterio \n",
        "# pip install spectral\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from time import time\n",
        "import rasterio as rio\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "from sklearn import cluster\n",
        "from sklearn.decomposition import PCA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Read the data"
      ],
      "metadata": {
        "id": "WztjJZpmXJX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the data\n",
        "data_raster = rio.open('drive/MyDrive/VAE_GeoChem/Delamerian_Landsat8.tif')\n",
        "# data_raster = rio.open('drive/MyDrive/VAE_GeoChem/Delamerian_ASTER.tif')\n",
        "\n",
        "# print(data_raster.meta)\n",
        "\n",
        "## Visualizing the data\n",
        "# Reading and enhancing\n",
        "data_array = data_raster.read() # reading the data\n",
        "# vmin, vmax = np.nanpercentile(data_array, (5,95)) # 5-95% pixel values stretch\n",
        "# Plotting the enhanced image\n",
        "# fig = plt.figure(figsize=[20,20])\n",
        "# plt.axis('off')\n",
        "# plt.imshow(data_array[1, :, :], cmap='gray', vmin=vmin, vmax=vmax)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "qGPpGtvFXHLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Reshape the hyper-spectoral image (HSI)"
      ],
      "metadata": {
        "id": "kHUObP2nXWxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Reshaping the input data from brc to rcb\n",
        "# Creating an empty array with the same dimension and data type\n",
        "imgxyb = np.empty((data_raster.height, data_raster.width, data_raster.count), data_raster.meta['dtype'])\n",
        "# Looping through the bands to fill the empty array\n",
        "for band in range(imgxyb.shape[2]):\n",
        "    imgxyb[:,:,band] = data_raster.read(band+1)\n",
        "\n",
        "# Reshaping the input data from rcb to samples and features\n",
        "data_reshaped = imgxyb.reshape(imgxyb.shape[0]*imgxyb.shape[1], -1)\n",
        "# Scaling\n",
        "data_reshaped = minmax_scale(data_reshaped, feature_range=(0, 1), axis=0, copy=False)\n",
        "# print(data_reshaped.shape)"
      ],
      "metadata": {
        "id": "J6DTR4MVXmHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Helping functions"
      ],
      "metadata": {
        "id": "yH0fKsWWXttK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to plot and display the image\n",
        "def plot_data(data):\n",
        "  fig = plt.figure(figsize = (15, 10))\n",
        "  plt.imshow(data, cmap = 'nipy_spectral')\n",
        "  plt.colorbar()\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "nuZk82cEXzX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Dimensionality Reduction: Principle Componenet Analysis. \n",
        "- Reduce the number of model parameters\n",
        "◦ Avoid over-fitting\n",
        "◦ Reduce the computational load.\n",
        "\n",
        "Data correlation and information redundancy.\n",
        "Signal-noise ratio maximization.\n",
        "\n",
        "\n",
        " 1. Subtract mean\n",
        " 2. Calculate the covariance matrix\n",
        " 3. Calculate eigenvectors and eigenvalues\n",
        "of the covariance matrix\n",
        " 4. Rank eigenvectors by its corresponding\n",
        "eigenvalues\n",
        " 4. Obtain P with its column vectors\n",
        "corresponding to the top k eigenvectors"
      ],
      "metadata": {
        "id": "VYt7gSHEX-Zo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA\n",
        "pca = PCA(n_components=data_array.shape[0])\n",
        "components = pca.fit_transform(data_reshaped)\n",
        "var_ratio = pca.explained_variance_ratio_\n",
        "values = pca.singular_values_\n",
        "\n",
        "# print(var_ratio.shape)\n",
        "# print(values)"
      ],
      "metadata": {
        "id": "VGn6X2ufX90J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Optimal number of components:"
      ],
      "metadata": {
        "id": "oDDknbzqajRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to calculate the wss loss to find out the optimal no of clusters\n",
        "# for a give number of compenents\n",
        "def calculate_WSS(points, kmax):\n",
        "  sse = []\n",
        "  for k in range(1, kmax+1):\n",
        "    kmeans = cluster.KMeans(n_clusters = k).fit(points)\n",
        "    centroids = kmeans.cluster_centers_\n",
        "    pred_clusters = kmeans.predict(points)\n",
        "    curr_sse = 0\n",
        "    \n",
        "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
        "    for i in range(len(points)):\n",
        "      curr_center = centroids[pred_clusters[i]]\n",
        "      curr_sse += (points[i, 0] - curr_center[0]) ** 2 + (points[i, 1] - curr_center[1]) ** 2\n",
        "      \n",
        "    sse.append(curr_sse)\n",
        "  return sse\n",
        "\n",
        "kmax            = 10\n",
        "components_num  = 5\n",
        "sse = calculate_WSS(components[:,:components_num], kmax)\n",
        "# plot the wss vs clusters \n",
        "plot_data(sse)"
      ],
      "metadata": {
        "id": "9gOZJeUMaigZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}